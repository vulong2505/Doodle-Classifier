{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ANN Doodle Classfier"
      ],
      "metadata": {
        "id": "nkxXBVwZSBr_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WXqeOQuR77d"
      },
      "outputs": [],
      "source": [
        "\"\"\" FCNN for Doodle Classification. \"\"\"\n",
        "\n",
        "##### Libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# =============================================================================\n",
        "# Network Architecture\n",
        "# =============================================================================\n",
        "\n",
        "class DoodleANN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        # Init\n",
        "        super(DoodleANN, self).__init__()\n",
        "\n",
        "        # Neurons\n",
        "        self.input_layer = 28*28 # features from flattened 28*28 image\n",
        "        self.hidden_layer_1 = 300 # neurons\n",
        "        self.hidden_layer_2 = 60 # neurons\n",
        "        self.hidden_layer_3 = 50 # neurons\n",
        "        self.hidden_layer_4 = 40 # neurons\n",
        "        self.output_layer = 10 # categories\n",
        "\n",
        "        # Architecture\n",
        "        self.fc1 = nn.Linear(self.input_layer, self.hidden_layer_1)\n",
        "        self.fc2 = nn.Linear(self.hidden_layer_1, self.hidden_layer_2)\n",
        "        self.fc3 = nn.Linear(self.hidden_layer_2, self.hidden_layer_3)\n",
        "        self.fc4 = nn.Linear(self.hidden_layer_3, self.hidden_layer_4)\n",
        "        self.output = nn.Linear(self.hidden_layer_4, self.output_layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flattening the input tensor\n",
        "        x = x.view(-1, self.input_layer)\n",
        "\n",
        "        x = F.relu(self.fc1(x))     # Relu activation function for hidden layer 1\n",
        "        x = F.relu(self.fc2(x))     # Relu activation function for hidden layer 2\n",
        "        x = F.relu(self.fc3(x))     # Relu activation function for hidden layer 3\n",
        "        x = F.relu(self.fc4(x))     # Relu activation function for hidden layer 4\n",
        "        x = self.output(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train DoodleANN"
      ],
      "metadata": {
        "id": "RW8k5UxNSJTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Train network here. Persistence to store weights. \"\"\"\n",
        "\n",
        "##### Libraries\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from google.colab import drive\n",
        "\n",
        "# =============================================================================\n",
        "# Directory Path\n",
        "# =============================================================================\n",
        "\n",
        "model_save_path = '/content/drive/My Drive/Your_Folder/doodle_ann_weights.pth'\n",
        "\n",
        "# =============================================================================\n",
        "# Train Network\n",
        "# =============================================================================\n",
        "\n",
        "## Drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "## Get data ready\n",
        "print(\"Getting data ready to train...\")\n",
        "df = pd.read_csv('/content/drive/My Drive/Doodle/doodle_dataframe.csv')\n",
        "pixel_columns = [f'pixel{i}' for i in range(28*28)]\n",
        "X = df[pixel_columns].values\n",
        "y = df['label'].values\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Label Encoder to change string categorical labels to numbers\n",
        "encoder = joblib.load('/content/doodle_category_encoder.joblib')\n",
        "y_train_encoded = encoder.transform(y_train)\n",
        "y_test_encoded = encoder.transform(y_test)\n",
        "\n",
        "## Train model\n",
        "print(\"Training...\")\n",
        "model = DoodleANN()\n",
        "# print(\"Loading weights...\")\n",
        "# model.load_state_dict(torch.load('/content/doodle_ann_weights.pth'))\n",
        "model.train()  # set model to training mode\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Prepare data\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)      # ensure long type for CrossEntropyLoss\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.long)\n",
        "\n",
        "# Training Loop\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "      # Save weights\n",
        "      print(\"Persistence: Saving model weights...\")\n",
        "      torch.save(model.state_dict(), '/content/doodle_ann_weights_100_4layrs.pth')\n",
        "\n",
        "# Save weights\n",
        "print(\"Finished training. Saving model weights!\")\n",
        "torch.save(model.state_dict(), '/content/doodle_ann_weights_100_4layers.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WNZ-FTlSb1Y",
        "outputId": "78067ffa-9f99-4e33-aca9-da9eeb26c632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "Getting data ready to train...\n",
            "Training...\n",
            "Epoch [0], Loss: 2.3063\n",
            "Persistence: Saving model weights...\n",
            "Epoch [1], Loss: 2.3024\n",
            "Epoch [2], Loss: 2.2983\n",
            "Epoch [3], Loss: 2.2931\n",
            "Epoch [4], Loss: 2.2871\n",
            "Epoch [5], Loss: 2.2798\n",
            "Epoch [6], Loss: 2.2714\n",
            "Epoch [7], Loss: 2.2611\n",
            "Epoch [8], Loss: 2.2484\n",
            "Epoch [9], Loss: 2.2336\n",
            "Epoch [10], Loss: 2.2162\n",
            "Persistence: Saving model weights...\n",
            "Epoch [11], Loss: 2.1961\n",
            "Epoch [12], Loss: 2.1731\n",
            "Epoch [13], Loss: 2.1479\n",
            "Epoch [14], Loss: 2.1206\n",
            "Epoch [15], Loss: 2.0915\n",
            "Epoch [16], Loss: 2.0617\n",
            "Epoch [17], Loss: 2.0314\n",
            "Epoch [18], Loss: 2.0010\n",
            "Epoch [19], Loss: 1.9714\n",
            "Epoch [20], Loss: 1.9434\n",
            "Persistence: Saving model weights...\n",
            "Epoch [21], Loss: 1.9174\n",
            "Epoch [22], Loss: 1.8938\n",
            "Epoch [23], Loss: 1.8728\n",
            "Epoch [24], Loss: 1.8552\n",
            "Epoch [25], Loss: 1.8403\n",
            "Epoch [26], Loss: 1.8264\n",
            "Epoch [27], Loss: 1.8142\n",
            "Epoch [28], Loss: 1.8028\n",
            "Epoch [29], Loss: 1.7904\n",
            "Epoch [30], Loss: 1.7767\n",
            "Persistence: Saving model weights...\n",
            "Epoch [31], Loss: 1.7630\n",
            "Epoch [32], Loss: 1.7502\n",
            "Epoch [33], Loss: 1.7370\n",
            "Epoch [34], Loss: 1.7240\n",
            "Epoch [35], Loss: 1.7127\n",
            "Epoch [36], Loss: 1.7021\n",
            "Epoch [37], Loss: 1.6916\n",
            "Epoch [38], Loss: 1.6821\n",
            "Epoch [39], Loss: 1.6734\n",
            "Epoch [40], Loss: 1.6637\n",
            "Persistence: Saving model weights...\n",
            "Epoch [41], Loss: 1.6539\n",
            "Epoch [42], Loss: 1.6447\n",
            "Epoch [43], Loss: 1.6350\n",
            "Epoch [44], Loss: 1.6248\n",
            "Epoch [45], Loss: 1.6153\n",
            "Epoch [46], Loss: 1.6063\n",
            "Epoch [47], Loss: 1.5970\n",
            "Epoch [48], Loss: 1.5865\n",
            "Epoch [49], Loss: 1.5777\n",
            "Epoch [50], Loss: 1.5701\n",
            "Persistence: Saving model weights...\n",
            "Epoch [51], Loss: 1.5622\n",
            "Epoch [52], Loss: 1.5542\n",
            "Epoch [53], Loss: 1.5469\n",
            "Epoch [54], Loss: 1.5414\n",
            "Epoch [55], Loss: 1.5348\n",
            "Epoch [56], Loss: 1.5255\n",
            "Epoch [57], Loss: 1.5168\n",
            "Epoch [58], Loss: 1.5114\n",
            "Epoch [59], Loss: 1.5052\n",
            "Epoch [60], Loss: 1.4966\n",
            "Persistence: Saving model weights...\n",
            "Epoch [61], Loss: 1.4902\n",
            "Epoch [62], Loss: 1.4860\n",
            "Epoch [63], Loss: 1.4797\n",
            "Epoch [64], Loss: 1.4724\n",
            "Epoch [65], Loss: 1.4687\n",
            "Epoch [66], Loss: 1.4635\n",
            "Epoch [67], Loss: 1.4544\n",
            "Epoch [68], Loss: 1.4481\n",
            "Epoch [69], Loss: 1.4428\n",
            "Epoch [70], Loss: 1.4360\n",
            "Persistence: Saving model weights...\n",
            "Epoch [71], Loss: 1.4322\n",
            "Epoch [72], Loss: 1.4282\n",
            "Epoch [73], Loss: 1.4209\n",
            "Epoch [74], Loss: 1.4153\n",
            "Epoch [75], Loss: 1.4100\n",
            "Epoch [76], Loss: 1.4031\n",
            "Epoch [77], Loss: 1.3986\n",
            "Epoch [78], Loss: 1.3948\n",
            "Epoch [79], Loss: 1.3900\n",
            "Epoch [80], Loss: 1.3871\n",
            "Persistence: Saving model weights...\n",
            "Epoch [81], Loss: 1.3866\n",
            "Epoch [82], Loss: 1.3826\n",
            "Epoch [83], Loss: 1.3751\n",
            "Epoch [84], Loss: 1.3673\n",
            "Epoch [85], Loss: 1.3620\n",
            "Epoch [86], Loss: 1.3612\n",
            "Epoch [87], Loss: 1.3587\n",
            "Epoch [88], Loss: 1.3507\n",
            "Epoch [89], Loss: 1.3451\n",
            "Epoch [90], Loss: 1.3441\n",
            "Persistence: Saving model weights...\n",
            "Epoch [91], Loss: 1.3411\n",
            "Epoch [92], Loss: 1.3355\n",
            "Epoch [93], Loss: 1.3309\n",
            "Epoch [94], Loss: 1.3271\n",
            "Epoch [95], Loss: 1.3244\n",
            "Epoch [96], Loss: 1.3215\n",
            "Epoch [97], Loss: 1.3163\n",
            "Epoch [98], Loss: 1.3116\n",
            "Epoch [99], Loss: 1.3093\n",
            "Epoch [100], Loss: 1.3067\n",
            "Persistence: Saving model weights...\n",
            "Epoch [101], Loss: 1.3023\n",
            "Epoch [102], Loss: 1.2982\n",
            "Epoch [103], Loss: 1.2947\n",
            "Epoch [104], Loss: 1.2910\n",
            "Epoch [105], Loss: 1.2876\n",
            "Epoch [106], Loss: 1.2848\n",
            "Epoch [107], Loss: 1.2816\n",
            "Epoch [108], Loss: 1.2775\n",
            "Epoch [109], Loss: 1.2733\n",
            "Epoch [110], Loss: 1.2698\n",
            "Persistence: Saving model weights...\n",
            "Epoch [111], Loss: 1.2665\n",
            "Epoch [112], Loss: 1.2631\n",
            "Epoch [113], Loss: 1.2595\n",
            "Epoch [114], Loss: 1.2562\n",
            "Epoch [115], Loss: 1.2533\n",
            "Epoch [116], Loss: 1.2506\n",
            "Epoch [117], Loss: 1.2478\n",
            "Epoch [118], Loss: 1.2448\n",
            "Epoch [119], Loss: 1.2417\n",
            "Epoch [120], Loss: 1.2386\n",
            "Persistence: Saving model weights...\n",
            "Epoch [121], Loss: 1.2355\n",
            "Epoch [122], Loss: 1.2321\n",
            "Epoch [123], Loss: 1.2283\n",
            "Epoch [124], Loss: 1.2240\n",
            "Epoch [125], Loss: 1.2199\n",
            "Epoch [126], Loss: 1.2165\n",
            "Epoch [127], Loss: 1.2139\n",
            "Epoch [128], Loss: 1.2116\n",
            "Epoch [129], Loss: 1.2090\n",
            "Epoch [130], Loss: 1.2058\n",
            "Persistence: Saving model weights...\n",
            "Epoch [131], Loss: 1.2019\n",
            "Epoch [132], Loss: 1.1980\n",
            "Epoch [133], Loss: 1.1944\n",
            "Epoch [134], Loss: 1.1914\n",
            "Epoch [135], Loss: 1.1888\n",
            "Epoch [136], Loss: 1.1864\n",
            "Epoch [137], Loss: 1.1840\n",
            "Epoch [138], Loss: 1.1815\n",
            "Epoch [139], Loss: 1.1785\n",
            "Epoch [140], Loss: 1.1751\n",
            "Persistence: Saving model weights...\n",
            "Epoch [141], Loss: 1.1714\n",
            "Epoch [142], Loss: 1.1677\n",
            "Epoch [143], Loss: 1.1643\n",
            "Epoch [144], Loss: 1.1612\n",
            "Epoch [145], Loss: 1.1586\n",
            "Epoch [146], Loss: 1.1563\n",
            "Epoch [147], Loss: 1.1545\n",
            "Epoch [148], Loss: 1.1534\n",
            "Epoch [149], Loss: 1.1534\n",
            "Epoch [150], Loss: 1.1526\n",
            "Persistence: Saving model weights...\n",
            "Epoch [151], Loss: 1.1496\n",
            "Epoch [152], Loss: 1.1425\n",
            "Epoch [153], Loss: 1.1359\n",
            "Epoch [154], Loss: 1.1335\n",
            "Epoch [155], Loss: 1.1340\n",
            "Epoch [156], Loss: 1.1332\n",
            "Epoch [157], Loss: 1.1285\n",
            "Epoch [158], Loss: 1.1230\n",
            "Epoch [159], Loss: 1.1199\n",
            "Epoch [160], Loss: 1.1190\n",
            "Persistence: Saving model weights...\n",
            "Epoch [161], Loss: 1.1180\n",
            "Epoch [162], Loss: 1.1146\n",
            "Epoch [163], Loss: 1.1102\n",
            "Epoch [164], Loss: 1.1068\n",
            "Epoch [165], Loss: 1.1051\n",
            "Epoch [166], Loss: 1.1038\n",
            "Epoch [167], Loss: 1.1019\n",
            "Epoch [168], Loss: 1.0990\n",
            "Epoch [169], Loss: 1.0955\n",
            "Epoch [170], Loss: 1.0921\n",
            "Persistence: Saving model weights...\n",
            "Epoch [171], Loss: 1.0893\n",
            "Epoch [172], Loss: 1.0873\n",
            "Epoch [173], Loss: 1.0857\n",
            "Epoch [174], Loss: 1.0842\n",
            "Epoch [175], Loss: 1.0824\n",
            "Epoch [176], Loss: 1.0803\n",
            "Epoch [177], Loss: 1.0775\n",
            "Epoch [178], Loss: 1.0742\n",
            "Epoch [179], Loss: 1.0708\n",
            "Epoch [180], Loss: 1.0678\n",
            "Persistence: Saving model weights...\n",
            "Epoch [181], Loss: 1.0659\n",
            "Epoch [182], Loss: 1.0654\n",
            "Epoch [183], Loss: 1.0681\n",
            "Epoch [184], Loss: 1.0765\n",
            "Epoch [185], Loss: 1.0898\n",
            "Epoch [186], Loss: 1.0811\n",
            "Epoch [187], Loss: 1.0674\n",
            "Epoch [188], Loss: 1.0645\n",
            "Epoch [189], Loss: 1.0640\n",
            "Epoch [190], Loss: 1.0601\n",
            "Persistence: Saving model weights...\n",
            "Epoch [191], Loss: 1.0611\n",
            "Epoch [192], Loss: 1.0530\n",
            "Epoch [193], Loss: 1.0514\n",
            "Epoch [194], Loss: 1.0573\n",
            "Epoch [195], Loss: 1.0423\n",
            "Epoch [196], Loss: 1.0482\n",
            "Epoch [197], Loss: 1.0454\n",
            "Epoch [198], Loss: 1.0373\n",
            "Epoch [199], Loss: 1.0420\n",
            "Epoch [200], Loss: 1.0346\n",
            "Persistence: Saving model weights...\n",
            "Epoch [201], Loss: 1.0356\n",
            "Epoch [202], Loss: 1.0309\n",
            "Epoch [203], Loss: 1.0313\n",
            "Epoch [204], Loss: 1.0278\n",
            "Epoch [205], Loss: 1.0247\n",
            "Epoch [206], Loss: 1.0263\n",
            "Epoch [207], Loss: 1.0203\n",
            "Epoch [208], Loss: 1.0217\n",
            "Epoch [209], Loss: 1.0179\n",
            "Epoch [210], Loss: 1.0180\n",
            "Persistence: Saving model weights...\n",
            "Epoch [211], Loss: 1.0143\n",
            "Epoch [212], Loss: 1.0137\n",
            "Epoch [213], Loss: 1.0123\n",
            "Epoch [214], Loss: 1.0092\n",
            "Epoch [215], Loss: 1.0089\n",
            "Epoch [216], Loss: 1.0062\n",
            "Epoch [217], Loss: 1.0055\n",
            "Epoch [218], Loss: 1.0029\n",
            "Epoch [219], Loss: 1.0016\n",
            "Epoch [220], Loss: 1.0005\n",
            "Persistence: Saving model weights...\n",
            "Epoch [221], Loss: 0.9980\n",
            "Epoch [222], Loss: 0.9972\n",
            "Epoch [223], Loss: 0.9948\n",
            "Epoch [224], Loss: 0.9940\n",
            "Epoch [225], Loss: 0.9922\n",
            "Epoch [226], Loss: 0.9904\n",
            "Epoch [227], Loss: 0.9891\n",
            "Epoch [228], Loss: 0.9871\n",
            "Epoch [229], Loss: 0.9861\n",
            "Epoch [230], Loss: 0.9844\n",
            "Persistence: Saving model weights...\n",
            "Epoch [231], Loss: 0.9828\n",
            "Epoch [232], Loss: 0.9813\n",
            "Epoch [233], Loss: 0.9795\n",
            "Epoch [234], Loss: 0.9782\n",
            "Epoch [235], Loss: 0.9766\n",
            "Epoch [236], Loss: 0.9752\n",
            "Epoch [237], Loss: 0.9739\n",
            "Epoch [238], Loss: 0.9722\n",
            "Epoch [239], Loss: 0.9709\n",
            "Epoch [240], Loss: 0.9696\n",
            "Persistence: Saving model weights...\n",
            "Epoch [241], Loss: 0.9688\n",
            "Epoch [242], Loss: 0.9693\n",
            "Epoch [243], Loss: 0.9702\n",
            "Epoch [244], Loss: 0.9728\n",
            "Epoch [245], Loss: 0.9739\n",
            "Epoch [246], Loss: 0.9736\n",
            "Epoch [247], Loss: 0.9664\n",
            "Epoch [248], Loss: 0.9592\n",
            "Epoch [249], Loss: 0.9568\n",
            "Epoch [250], Loss: 0.9591\n",
            "Persistence: Saving model weights...\n",
            "Epoch [251], Loss: 0.9617\n",
            "Epoch [252], Loss: 0.9587\n",
            "Epoch [253], Loss: 0.9532\n",
            "Epoch [254], Loss: 0.9498\n",
            "Epoch [255], Loss: 0.9509\n",
            "Epoch [256], Loss: 0.9524\n",
            "Epoch [257], Loss: 0.9501\n",
            "Epoch [258], Loss: 0.9458\n",
            "Epoch [259], Loss: 0.9434\n",
            "Epoch [260], Loss: 0.9437\n",
            "Persistence: Saving model weights...\n",
            "Epoch [261], Loss: 0.9442\n",
            "Epoch [262], Loss: 0.9422\n",
            "Epoch [263], Loss: 0.9391\n",
            "Epoch [264], Loss: 0.9369\n",
            "Epoch [265], Loss: 0.9366\n",
            "Epoch [266], Loss: 0.9365\n",
            "Epoch [267], Loss: 0.9352\n",
            "Epoch [268], Loss: 0.9329\n",
            "Epoch [269], Loss: 0.9307\n",
            "Epoch [270], Loss: 0.9294\n",
            "Persistence: Saving model weights...\n",
            "Epoch [271], Loss: 0.9288\n",
            "Epoch [272], Loss: 0.9282\n",
            "Epoch [273], Loss: 0.9271\n",
            "Epoch [274], Loss: 0.9254\n",
            "Epoch [275], Loss: 0.9236\n",
            "Epoch [276], Loss: 0.9219\n",
            "Epoch [277], Loss: 0.9205\n",
            "Epoch [278], Loss: 0.9195\n",
            "Epoch [279], Loss: 0.9186\n",
            "Epoch [280], Loss: 0.9177\n",
            "Persistence: Saving model weights...\n",
            "Epoch [281], Loss: 0.9168\n",
            "Epoch [282], Loss: 0.9159\n",
            "Epoch [283], Loss: 0.9149\n",
            "Epoch [284], Loss: 0.9140\n",
            "Epoch [285], Loss: 0.9129\n",
            "Epoch [286], Loss: 0.9119\n",
            "Epoch [287], Loss: 0.9107\n",
            "Epoch [288], Loss: 0.9098\n",
            "Epoch [289], Loss: 0.9085\n",
            "Epoch [290], Loss: 0.9075\n",
            "Persistence: Saving model weights...\n",
            "Epoch [291], Loss: 0.9061\n",
            "Epoch [292], Loss: 0.9048\n",
            "Epoch [293], Loss: 0.9032\n",
            "Epoch [294], Loss: 0.9016\n",
            "Epoch [295], Loss: 0.8999\n",
            "Epoch [296], Loss: 0.8983\n",
            "Epoch [297], Loss: 0.8967\n",
            "Epoch [298], Loss: 0.8953\n",
            "Epoch [299], Loss: 0.8940\n",
            "Epoch [300], Loss: 0.8927\n",
            "Persistence: Saving model weights...\n",
            "Epoch [301], Loss: 0.8915\n",
            "Epoch [302], Loss: 0.8902\n",
            "Epoch [303], Loss: 0.8890\n",
            "Epoch [304], Loss: 0.8878\n",
            "Epoch [305], Loss: 0.8866\n",
            "Epoch [306], Loss: 0.8854\n",
            "Epoch [307], Loss: 0.8843\n",
            "Epoch [308], Loss: 0.8831\n",
            "Epoch [309], Loss: 0.8821\n",
            "Epoch [310], Loss: 0.8812\n",
            "Persistence: Saving model weights...\n",
            "Epoch [311], Loss: 0.8809\n",
            "Epoch [312], Loss: 0.8823\n",
            "Epoch [313], Loss: 0.8886\n",
            "Epoch [314], Loss: 0.9029\n",
            "Epoch [315], Loss: 0.9318\n",
            "Epoch [316], Loss: 0.9293\n",
            "Epoch [317], Loss: 0.9007\n",
            "Epoch [318], Loss: 0.8785\n",
            "Epoch [319], Loss: 0.9013\n",
            "Epoch [320], Loss: 0.9061\n",
            "Persistence: Saving model weights...\n",
            "Epoch [321], Loss: 0.8801\n",
            "Epoch [322], Loss: 0.8925\n",
            "Epoch [323], Loss: 0.8928\n",
            "Epoch [324], Loss: 0.8749\n",
            "Epoch [325], Loss: 0.8882\n",
            "Epoch [326], Loss: 0.8782\n",
            "Epoch [327], Loss: 0.8754\n",
            "Epoch [328], Loss: 0.8808\n",
            "Epoch [329], Loss: 0.8680\n",
            "Epoch [330], Loss: 0.8739\n",
            "Persistence: Saving model weights...\n",
            "Epoch [331], Loss: 0.8686\n",
            "Epoch [332], Loss: 0.8664\n",
            "Epoch [333], Loss: 0.8698\n",
            "Epoch [334], Loss: 0.8618\n",
            "Epoch [335], Loss: 0.8655\n",
            "Epoch [336], Loss: 0.8614\n",
            "Epoch [337], Loss: 0.8597\n",
            "Epoch [338], Loss: 0.8609\n",
            "Epoch [339], Loss: 0.8561\n",
            "Epoch [340], Loss: 0.8581\n",
            "Persistence: Saving model weights...\n",
            "Epoch [341], Loss: 0.8552\n",
            "Epoch [342], Loss: 0.8542\n",
            "Epoch [343], Loss: 0.8547\n",
            "Epoch [344], Loss: 0.8512\n",
            "Epoch [345], Loss: 0.8525\n",
            "Epoch [346], Loss: 0.8502\n",
            "Epoch [347], Loss: 0.8488\n",
            "Epoch [348], Loss: 0.8493\n",
            "Epoch [349], Loss: 0.8465\n",
            "Epoch [350], Loss: 0.8467\n",
            "Persistence: Saving model weights...\n",
            "Epoch [351], Loss: 0.8456\n",
            "Epoch [352], Loss: 0.8436\n",
            "Epoch [353], Loss: 0.8441\n",
            "Epoch [354], Loss: 0.8421\n",
            "Epoch [355], Loss: 0.8412\n",
            "Epoch [356], Loss: 0.8410\n",
            "Epoch [357], Loss: 0.8391\n",
            "Epoch [358], Loss: 0.8387\n",
            "Epoch [359], Loss: 0.8379\n",
            "Epoch [360], Loss: 0.8364\n",
            "Persistence: Saving model weights...\n",
            "Epoch [361], Loss: 0.8360\n",
            "Epoch [362], Loss: 0.8350\n",
            "Epoch [363], Loss: 0.8338\n",
            "Epoch [364], Loss: 0.8332\n",
            "Epoch [365], Loss: 0.8321\n",
            "Epoch [366], Loss: 0.8311\n",
            "Epoch [367], Loss: 0.8304\n",
            "Epoch [368], Loss: 0.8294\n",
            "Epoch [369], Loss: 0.8284\n",
            "Epoch [370], Loss: 0.8276\n",
            "Persistence: Saving model weights...\n",
            "Epoch [371], Loss: 0.8267\n",
            "Epoch [372], Loss: 0.8257\n",
            "Epoch [373], Loss: 0.8249\n",
            "Epoch [374], Loss: 0.8240\n",
            "Epoch [375], Loss: 0.8230\n",
            "Epoch [376], Loss: 0.8221\n",
            "Epoch [377], Loss: 0.8213\n",
            "Epoch [378], Loss: 0.8203\n",
            "Epoch [379], Loss: 0.8194\n",
            "Epoch [380], Loss: 0.8187\n",
            "Persistence: Saving model weights...\n",
            "Epoch [381], Loss: 0.8178\n",
            "Epoch [382], Loss: 0.8170\n",
            "Epoch [383], Loss: 0.8164\n",
            "Epoch [384], Loss: 0.8160\n",
            "Epoch [385], Loss: 0.8158\n",
            "Epoch [386], Loss: 0.8158\n",
            "Epoch [387], Loss: 0.8157\n",
            "Epoch [388], Loss: 0.8149\n",
            "Epoch [389], Loss: 0.8136\n",
            "Epoch [390], Loss: 0.8117\n",
            "Persistence: Saving model weights...\n",
            "Epoch [391], Loss: 0.8101\n",
            "Epoch [392], Loss: 0.8089\n",
            "Epoch [393], Loss: 0.8084\n",
            "Epoch [394], Loss: 0.8079\n",
            "Epoch [395], Loss: 0.8073\n",
            "Epoch [396], Loss: 0.8063\n",
            "Epoch [397], Loss: 0.8052\n",
            "Epoch [398], Loss: 0.8042\n",
            "Epoch [399], Loss: 0.8034\n",
            "Epoch [400], Loss: 0.8028\n",
            "Persistence: Saving model weights...\n",
            "Epoch [401], Loss: 0.8024\n",
            "Epoch [402], Loss: 0.8019\n",
            "Epoch [403], Loss: 0.8011\n",
            "Epoch [404], Loss: 0.8000\n",
            "Epoch [405], Loss: 0.7987\n",
            "Epoch [406], Loss: 0.7975\n",
            "Epoch [407], Loss: 0.7965\n",
            "Epoch [408], Loss: 0.7958\n",
            "Epoch [409], Loss: 0.7953\n",
            "Epoch [410], Loss: 0.7948\n",
            "Persistence: Saving model weights...\n",
            "Epoch [411], Loss: 0.7944\n",
            "Epoch [412], Loss: 0.7940\n",
            "Epoch [413], Loss: 0.7936\n",
            "Epoch [414], Loss: 0.7931\n",
            "Epoch [415], Loss: 0.7925\n",
            "Epoch [416], Loss: 0.7919\n",
            "Epoch [417], Loss: 0.7914\n",
            "Epoch [418], Loss: 0.7910\n",
            "Epoch [419], Loss: 0.7915\n",
            "Epoch [420], Loss: 0.7919\n",
            "Persistence: Saving model weights...\n",
            "Epoch [421], Loss: 0.7933\n",
            "Epoch [422], Loss: 0.7928\n",
            "Epoch [423], Loss: 0.7909\n",
            "Epoch [424], Loss: 0.7873\n",
            "Epoch [425], Loss: 0.7846\n",
            "Epoch [426], Loss: 0.7842\n",
            "Epoch [427], Loss: 0.7849\n",
            "Epoch [428], Loss: 0.7851\n",
            "Epoch [429], Loss: 0.7829\n",
            "Epoch [430], Loss: 0.7806\n",
            "Persistence: Saving model weights...\n",
            "Epoch [431], Loss: 0.7798\n",
            "Epoch [432], Loss: 0.7803\n",
            "Epoch [433], Loss: 0.7808\n",
            "Epoch [434], Loss: 0.7795\n",
            "Epoch [435], Loss: 0.7776\n",
            "Epoch [436], Loss: 0.7760\n",
            "Epoch [437], Loss: 0.7756\n",
            "Epoch [438], Loss: 0.7758\n",
            "Epoch [439], Loss: 0.7755\n",
            "Epoch [440], Loss: 0.7747\n",
            "Persistence: Saving model weights...\n",
            "Epoch [441], Loss: 0.7734\n",
            "Epoch [442], Loss: 0.7724\n",
            "Epoch [443], Loss: 0.7719\n",
            "Epoch [444], Loss: 0.7717\n",
            "Epoch [445], Loss: 0.7715\n",
            "Epoch [446], Loss: 0.7709\n",
            "Epoch [447], Loss: 0.7700\n",
            "Epoch [448], Loss: 0.7692\n",
            "Epoch [449], Loss: 0.7686\n",
            "Epoch [450], Loss: 0.7685\n",
            "Persistence: Saving model weights...\n",
            "Epoch [451], Loss: 0.7687\n",
            "Epoch [452], Loss: 0.7691\n",
            "Epoch [453], Loss: 0.7692\n",
            "Epoch [454], Loss: 0.7688\n",
            "Epoch [455], Loss: 0.7675\n",
            "Epoch [456], Loss: 0.7657\n",
            "Epoch [457], Loss: 0.7641\n",
            "Epoch [458], Loss: 0.7633\n",
            "Epoch [459], Loss: 0.7630\n",
            "Epoch [460], Loss: 0.7627\n",
            "Persistence: Saving model weights...\n",
            "Epoch [461], Loss: 0.7621\n",
            "Epoch [462], Loss: 0.7609\n",
            "Epoch [463], Loss: 0.7598\n",
            "Epoch [464], Loss: 0.7591\n",
            "Epoch [465], Loss: 0.7587\n",
            "Epoch [466], Loss: 0.7587\n",
            "Epoch [467], Loss: 0.7585\n",
            "Epoch [468], Loss: 0.7582\n",
            "Epoch [469], Loss: 0.7573\n",
            "Epoch [470], Loss: 0.7563\n",
            "Persistence: Saving model weights...\n",
            "Epoch [471], Loss: 0.7553\n",
            "Epoch [472], Loss: 0.7545\n",
            "Epoch [473], Loss: 0.7540\n",
            "Epoch [474], Loss: 0.7538\n",
            "Epoch [475], Loss: 0.7538\n",
            "Epoch [476], Loss: 0.7538\n",
            "Epoch [477], Loss: 0.7540\n",
            "Epoch [478], Loss: 0.7539\n",
            "Epoch [479], Loss: 0.7537\n",
            "Epoch [480], Loss: 0.7530\n",
            "Persistence: Saving model weights...\n",
            "Epoch [481], Loss: 0.7522\n",
            "Epoch [482], Loss: 0.7515\n",
            "Epoch [483], Loss: 0.7514\n",
            "Epoch [484], Loss: 0.7517\n",
            "Epoch [485], Loss: 0.7522\n",
            "Epoch [486], Loss: 0.7520\n",
            "Epoch [487], Loss: 0.7504\n",
            "Epoch [488], Loss: 0.7479\n",
            "Epoch [489], Loss: 0.7455\n",
            "Epoch [490], Loss: 0.7443\n",
            "Persistence: Saving model weights...\n",
            "Epoch [491], Loss: 0.7443\n",
            "Epoch [492], Loss: 0.7448\n",
            "Epoch [493], Loss: 0.7450\n",
            "Epoch [494], Loss: 0.7442\n",
            "Epoch [495], Loss: 0.7429\n",
            "Epoch [496], Loss: 0.7417\n",
            "Epoch [497], Loss: 0.7410\n",
            "Epoch [498], Loss: 0.7406\n",
            "Epoch [499], Loss: 0.7405\n",
            "Epoch [500], Loss: 0.7404\n",
            "Persistence: Saving model weights...\n",
            "Epoch [501], Loss: 0.7399\n",
            "Epoch [502], Loss: 0.7393\n",
            "Epoch [503], Loss: 0.7386\n",
            "Epoch [504], Loss: 0.7380\n",
            "Epoch [505], Loss: 0.7375\n",
            "Epoch [506], Loss: 0.7372\n",
            "Epoch [507], Loss: 0.7371\n",
            "Epoch [508], Loss: 0.7372\n",
            "Epoch [509], Loss: 0.7375\n",
            "Epoch [510], Loss: 0.7382\n",
            "Persistence: Saving model weights...\n",
            "Epoch [511], Loss: 0.7391\n",
            "Epoch [512], Loss: 0.7406\n",
            "Epoch [513], Loss: 0.7420\n",
            "Epoch [514], Loss: 0.7431\n",
            "Epoch [515], Loss: 0.7429\n",
            "Epoch [516], Loss: 0.7403\n",
            "Epoch [517], Loss: 0.7376\n",
            "Epoch [518], Loss: 0.7342\n",
            "Epoch [519], Loss: 0.7326\n",
            "Epoch [520], Loss: 0.7324\n",
            "Persistence: Saving model weights...\n",
            "Epoch [521], Loss: 0.7328\n",
            "Epoch [522], Loss: 0.7327\n",
            "Epoch [523], Loss: 0.7315\n",
            "Epoch [524], Loss: 0.7300\n",
            "Epoch [525], Loss: 0.7288\n",
            "Epoch [526], Loss: 0.7283\n",
            "Epoch [527], Loss: 0.7282\n",
            "Epoch [528], Loss: 0.7281\n",
            "Epoch [529], Loss: 0.7275\n",
            "Epoch [530], Loss: 0.7264\n",
            "Persistence: Saving model weights...\n",
            "Epoch [531], Loss: 0.7253\n",
            "Epoch [532], Loss: 0.7247\n",
            "Epoch [533], Loss: 0.7245\n",
            "Epoch [534], Loss: 0.7244\n",
            "Epoch [535], Loss: 0.7239\n",
            "Epoch [536], Loss: 0.7231\n",
            "Epoch [537], Loss: 0.7222\n",
            "Epoch [538], Loss: 0.7216\n",
            "Epoch [539], Loss: 0.7212\n",
            "Epoch [540], Loss: 0.7209\n",
            "Persistence: Saving model weights...\n",
            "Epoch [541], Loss: 0.7208\n",
            "Epoch [542], Loss: 0.7209\n",
            "Epoch [543], Loss: 0.7213\n",
            "Epoch [544], Loss: 0.7224\n",
            "Epoch [545], Loss: 0.7247\n",
            "Epoch [546], Loss: 0.7286\n",
            "Epoch [547], Loss: 0.7342\n",
            "Epoch [548], Loss: 0.7363\n",
            "Epoch [549], Loss: 0.7319\n",
            "Epoch [550], Loss: 0.7212\n",
            "Persistence: Saving model weights...\n",
            "Epoch [551], Loss: 0.7170\n",
            "Epoch [552], Loss: 0.7214\n",
            "Epoch [553], Loss: 0.7234\n",
            "Epoch [554], Loss: 0.7191\n",
            "Epoch [555], Loss: 0.7156\n",
            "Epoch [556], Loss: 0.7174\n",
            "Epoch [557], Loss: 0.7193\n",
            "Epoch [558], Loss: 0.7159\n",
            "Epoch [559], Loss: 0.7132\n",
            "Epoch [560], Loss: 0.7148\n",
            "Persistence: Saving model weights...\n",
            "Epoch [561], Loss: 0.7158\n",
            "Epoch [562], Loss: 0.7141\n",
            "Epoch [563], Loss: 0.7120\n",
            "Epoch [564], Loss: 0.7116\n",
            "Epoch [565], Loss: 0.7122\n",
            "Epoch [566], Loss: 0.7118\n",
            "Epoch [567], Loss: 0.7105\n",
            "Epoch [568], Loss: 0.7096\n",
            "Epoch [569], Loss: 0.7098\n",
            "Epoch [570], Loss: 0.7098\n",
            "Persistence: Saving model weights...\n",
            "Epoch [571], Loss: 0.7089\n",
            "Epoch [572], Loss: 0.7074\n",
            "Epoch [573], Loss: 0.7071\n",
            "Epoch [574], Loss: 0.7075\n",
            "Epoch [575], Loss: 0.7072\n",
            "Epoch [576], Loss: 0.7061\n",
            "Epoch [577], Loss: 0.7053\n",
            "Epoch [578], Loss: 0.7054\n",
            "Epoch [579], Loss: 0.7054\n",
            "Epoch [580], Loss: 0.7048\n",
            "Persistence: Saving model weights...\n",
            "Epoch [581], Loss: 0.7039\n",
            "Epoch [582], Loss: 0.7034\n",
            "Epoch [583], Loss: 0.7032\n",
            "Epoch [584], Loss: 0.7030\n",
            "Epoch [585], Loss: 0.7026\n",
            "Epoch [586], Loss: 0.7021\n",
            "Epoch [587], Loss: 0.7016\n",
            "Epoch [588], Loss: 0.7012\n",
            "Epoch [589], Loss: 0.7009\n",
            "Epoch [590], Loss: 0.7009\n",
            "Persistence: Saving model weights...\n",
            "Epoch [591], Loss: 0.7009\n",
            "Epoch [592], Loss: 0.7012\n",
            "Epoch [593], Loss: 0.7017\n",
            "Epoch [594], Loss: 0.7031\n",
            "Epoch [595], Loss: 0.7058\n",
            "Epoch [596], Loss: 0.7101\n",
            "Epoch [597], Loss: 0.7142\n",
            "Epoch [598], Loss: 0.7166\n",
            "Epoch [599], Loss: 0.7123\n",
            "Epoch [600], Loss: 0.7044\n",
            "Persistence: Saving model weights...\n",
            "Epoch [601], Loss: 0.6984\n",
            "Epoch [602], Loss: 0.6996\n",
            "Epoch [603], Loss: 0.7026\n",
            "Epoch [604], Loss: 0.7015\n",
            "Epoch [605], Loss: 0.6971\n",
            "Epoch [606], Loss: 0.6957\n",
            "Epoch [607], Loss: 0.6984\n",
            "Epoch [608], Loss: 0.6987\n",
            "Epoch [609], Loss: 0.6958\n",
            "Epoch [610], Loss: 0.6930\n",
            "Persistence: Saving model weights...\n",
            "Epoch [611], Loss: 0.6939\n",
            "Epoch [612], Loss: 0.6960\n",
            "Epoch [613], Loss: 0.6948\n",
            "Epoch [614], Loss: 0.6922\n",
            "Epoch [615], Loss: 0.6910\n",
            "Epoch [616], Loss: 0.6919\n",
            "Epoch [617], Loss: 0.6927\n",
            "Epoch [618], Loss: 0.6915\n",
            "Epoch [619], Loss: 0.6898\n",
            "Epoch [620], Loss: 0.6892\n",
            "Persistence: Saving model weights...\n",
            "Epoch [621], Loss: 0.6896\n",
            "Epoch [622], Loss: 0.6898\n",
            "Epoch [623], Loss: 0.6891\n",
            "Epoch [624], Loss: 0.6880\n",
            "Epoch [625], Loss: 0.6874\n",
            "Epoch [626], Loss: 0.6873\n",
            "Epoch [627], Loss: 0.6874\n",
            "Epoch [628], Loss: 0.6869\n",
            "Epoch [629], Loss: 0.6862\n",
            "Epoch [630], Loss: 0.6855\n",
            "Persistence: Saving model weights...\n",
            "Epoch [631], Loss: 0.6852\n",
            "Epoch [632], Loss: 0.6851\n",
            "Epoch [633], Loss: 0.6849\n",
            "Epoch [634], Loss: 0.6847\n",
            "Epoch [635], Loss: 0.6841\n",
            "Epoch [636], Loss: 0.6836\n",
            "Epoch [637], Loss: 0.6832\n",
            "Epoch [638], Loss: 0.6830\n",
            "Epoch [639], Loss: 0.6829\n",
            "Epoch [640], Loss: 0.6828\n",
            "Persistence: Saving model weights...\n",
            "Epoch [641], Loss: 0.6827\n",
            "Epoch [642], Loss: 0.6825\n",
            "Epoch [643], Loss: 0.6825\n",
            "Epoch [644], Loss: 0.6827\n",
            "Epoch [645], Loss: 0.6833\n",
            "Epoch [646], Loss: 0.6839\n",
            "Epoch [647], Loss: 0.6848\n",
            "Epoch [648], Loss: 0.6850\n",
            "Epoch [649], Loss: 0.6851\n",
            "Epoch [650], Loss: 0.6836\n",
            "Persistence: Saving model weights...\n",
            "Epoch [651], Loss: 0.6820\n",
            "Epoch [652], Loss: 0.6801\n",
            "Epoch [653], Loss: 0.6792\n",
            "Epoch [654], Loss: 0.6792\n",
            "Epoch [655], Loss: 0.6800\n",
            "Epoch [656], Loss: 0.6808\n",
            "Epoch [657], Loss: 0.6808\n",
            "Epoch [658], Loss: 0.6800\n",
            "Epoch [659], Loss: 0.6782\n",
            "Epoch [660], Loss: 0.6766\n",
            "Persistence: Saving model weights...\n",
            "Epoch [661], Loss: 0.6758\n",
            "Epoch [662], Loss: 0.6757\n",
            "Epoch [663], Loss: 0.6759\n",
            "Epoch [664], Loss: 0.6758\n",
            "Epoch [665], Loss: 0.6752\n",
            "Epoch [666], Loss: 0.6744\n",
            "Epoch [667], Loss: 0.6738\n",
            "Epoch [668], Loss: 0.6735\n",
            "Epoch [669], Loss: 0.6735\n",
            "Epoch [670], Loss: 0.6736\n",
            "Persistence: Saving model weights...\n",
            "Epoch [671], Loss: 0.6735\n",
            "Epoch [672], Loss: 0.6731\n",
            "Epoch [673], Loss: 0.6725\n",
            "Epoch [674], Loss: 0.6719\n",
            "Epoch [675], Loss: 0.6714\n",
            "Epoch [676], Loss: 0.6711\n",
            "Epoch [677], Loss: 0.6710\n",
            "Epoch [678], Loss: 0.6712\n",
            "Epoch [679], Loss: 0.6716\n",
            "Epoch [680], Loss: 0.6720\n",
            "Persistence: Saving model weights...\n",
            "Epoch [681], Loss: 0.6726\n",
            "Epoch [682], Loss: 0.6732\n",
            "Epoch [683], Loss: 0.6741\n",
            "Epoch [684], Loss: 0.6751\n",
            "Epoch [685], Loss: 0.6763\n",
            "Epoch [686], Loss: 0.6773\n",
            "Epoch [687], Loss: 0.6768\n",
            "Epoch [688], Loss: 0.6754\n",
            "Epoch [689], Loss: 0.6716\n",
            "Epoch [690], Loss: 0.6684\n",
            "Persistence: Saving model weights...\n",
            "Epoch [691], Loss: 0.6670\n",
            "Epoch [692], Loss: 0.6679\n",
            "Epoch [693], Loss: 0.6699\n",
            "Epoch [694], Loss: 0.6706\n",
            "Epoch [695], Loss: 0.6700\n",
            "Epoch [696], Loss: 0.6677\n",
            "Epoch [697], Loss: 0.6660\n",
            "Epoch [698], Loss: 0.6658\n",
            "Epoch [699], Loss: 0.6667\n",
            "Epoch [700], Loss: 0.6676\n",
            "Persistence: Saving model weights...\n",
            "Epoch [701], Loss: 0.6673\n",
            "Epoch [702], Loss: 0.6657\n",
            "Epoch [703], Loss: 0.6640\n",
            "Epoch [704], Loss: 0.6629\n",
            "Epoch [705], Loss: 0.6628\n",
            "Epoch [706], Loss: 0.6631\n",
            "Epoch [707], Loss: 0.6631\n",
            "Epoch [708], Loss: 0.6624\n",
            "Epoch [709], Loss: 0.6614\n",
            "Epoch [710], Loss: 0.6606\n",
            "Persistence: Saving model weights...\n",
            "Epoch [711], Loss: 0.6602\n",
            "Epoch [712], Loss: 0.6603\n",
            "Epoch [713], Loss: 0.6603\n",
            "Epoch [714], Loss: 0.6601\n",
            "Epoch [715], Loss: 0.6596\n",
            "Epoch [716], Loss: 0.6590\n",
            "Epoch [717], Loss: 0.6585\n",
            "Epoch [718], Loss: 0.6581\n",
            "Epoch [719], Loss: 0.6580\n",
            "Epoch [720], Loss: 0.6580\n",
            "Persistence: Saving model weights...\n",
            "Epoch [721], Loss: 0.6582\n",
            "Epoch [722], Loss: 0.6584\n",
            "Epoch [723], Loss: 0.6591\n",
            "Epoch [724], Loss: 0.6606\n",
            "Epoch [725], Loss: 0.6638\n",
            "Epoch [726], Loss: 0.6682\n",
            "Epoch [727], Loss: 0.6753\n",
            "Epoch [728], Loss: 0.6754\n",
            "Epoch [729], Loss: 0.6704\n",
            "Epoch [730], Loss: 0.6591\n",
            "Persistence: Saving model weights...\n",
            "Epoch [731], Loss: 0.6573\n",
            "Epoch [732], Loss: 0.6635\n",
            "Epoch [733], Loss: 0.6650\n",
            "Epoch [734], Loss: 0.6587\n",
            "Epoch [735], Loss: 0.6541\n",
            "Epoch [736], Loss: 0.6573\n",
            "Epoch [737], Loss: 0.6607\n",
            "Epoch [738], Loss: 0.6571\n",
            "Epoch [739], Loss: 0.6530\n",
            "Epoch [740], Loss: 0.6551\n",
            "Persistence: Saving model weights...\n",
            "Epoch [741], Loss: 0.6571\n",
            "Epoch [742], Loss: 0.6549\n",
            "Epoch [743], Loss: 0.6520\n",
            "Epoch [744], Loss: 0.6530\n",
            "Epoch [745], Loss: 0.6547\n",
            "Epoch [746], Loss: 0.6531\n",
            "Epoch [747], Loss: 0.6509\n",
            "Epoch [748], Loss: 0.6509\n",
            "Epoch [749], Loss: 0.6522\n",
            "Epoch [750], Loss: 0.6521\n",
            "Persistence: Saving model weights...\n",
            "Epoch [751], Loss: 0.6502\n",
            "Epoch [752], Loss: 0.6493\n",
            "Epoch [753], Loss: 0.6501\n",
            "Epoch [754], Loss: 0.6504\n",
            "Epoch [755], Loss: 0.6495\n",
            "Epoch [756], Loss: 0.6483\n",
            "Epoch [757], Loss: 0.6482\n",
            "Epoch [758], Loss: 0.6487\n",
            "Epoch [759], Loss: 0.6486\n",
            "Epoch [760], Loss: 0.6477\n",
            "Persistence: Saving model weights...\n",
            "Epoch [761], Loss: 0.6470\n",
            "Epoch [762], Loss: 0.6471\n",
            "Epoch [763], Loss: 0.6475\n",
            "Epoch [764], Loss: 0.6474\n",
            "Epoch [765], Loss: 0.6473\n",
            "Epoch [766], Loss: 0.6476\n",
            "Epoch [767], Loss: 0.6489\n",
            "Epoch [768], Loss: 0.6512\n",
            "Epoch [769], Loss: 0.6542\n",
            "Epoch [770], Loss: 0.6579\n",
            "Persistence: Saving model weights...\n",
            "Epoch [771], Loss: 0.6614\n",
            "Epoch [772], Loss: 0.6618\n",
            "Epoch [773], Loss: 0.6572\n",
            "Epoch [774], Loss: 0.6495\n",
            "Epoch [775], Loss: 0.6448\n",
            "Epoch [776], Loss: 0.6462\n",
            "Epoch [777], Loss: 0.6494\n",
            "Epoch [778], Loss: 0.6496\n",
            "Epoch [779], Loss: 0.6458\n",
            "Epoch [780], Loss: 0.6432\n",
            "Persistence: Saving model weights...\n",
            "Epoch [781], Loss: 0.6445\n",
            "Epoch [782], Loss: 0.6460\n",
            "Epoch [783], Loss: 0.6454\n",
            "Epoch [784], Loss: 0.6427\n",
            "Epoch [785], Loss: 0.6415\n",
            "Epoch [786], Loss: 0.6424\n",
            "Epoch [787], Loss: 0.6434\n",
            "Epoch [788], Loss: 0.6430\n",
            "Epoch [789], Loss: 0.6412\n",
            "Epoch [790], Loss: 0.6400\n",
            "Persistence: Saving model weights...\n",
            "Epoch [791], Loss: 0.6403\n",
            "Epoch [792], Loss: 0.6409\n",
            "Epoch [793], Loss: 0.6408\n",
            "Epoch [794], Loss: 0.6398\n",
            "Epoch [795], Loss: 0.6389\n",
            "Epoch [796], Loss: 0.6387\n",
            "Epoch [797], Loss: 0.6389\n",
            "Epoch [798], Loss: 0.6390\n",
            "Epoch [799], Loss: 0.6385\n",
            "Epoch [800], Loss: 0.6378\n",
            "Persistence: Saving model weights...\n",
            "Epoch [801], Loss: 0.6373\n",
            "Epoch [802], Loss: 0.6371\n",
            "Epoch [803], Loss: 0.6372\n",
            "Epoch [804], Loss: 0.6370\n",
            "Epoch [805], Loss: 0.6367\n",
            "Epoch [806], Loss: 0.6362\n",
            "Epoch [807], Loss: 0.6359\n",
            "Epoch [808], Loss: 0.6357\n",
            "Epoch [809], Loss: 0.6355\n",
            "Epoch [810], Loss: 0.6354\n",
            "Persistence: Saving model weights...\n",
            "Epoch [811], Loss: 0.6352\n",
            "Epoch [812], Loss: 0.6349\n",
            "Epoch [813], Loss: 0.6345\n",
            "Epoch [814], Loss: 0.6343\n",
            "Epoch [815], Loss: 0.6341\n",
            "Epoch [816], Loss: 0.6339\n",
            "Epoch [817], Loss: 0.6339\n",
            "Epoch [818], Loss: 0.6338\n",
            "Epoch [819], Loss: 0.6338\n",
            "Epoch [820], Loss: 0.6340\n",
            "Persistence: Saving model weights...\n",
            "Epoch [821], Loss: 0.6343\n",
            "Epoch [822], Loss: 0.6350\n",
            "Epoch [823], Loss: 0.6364\n",
            "Epoch [824], Loss: 0.6380\n",
            "Epoch [825], Loss: 0.6407\n",
            "Epoch [826], Loss: 0.6423\n",
            "Epoch [827], Loss: 0.6438\n",
            "Epoch [828], Loss: 0.6414\n",
            "Epoch [829], Loss: 0.6376\n",
            "Epoch [830], Loss: 0.6329\n",
            "Persistence: Saving model weights...\n",
            "Epoch [831], Loss: 0.6308\n",
            "Epoch [832], Loss: 0.6317\n",
            "Epoch [833], Loss: 0.6338\n",
            "Epoch [834], Loss: 0.6347\n",
            "Epoch [835], Loss: 0.6330\n",
            "Epoch [836], Loss: 0.6307\n",
            "Epoch [837], Loss: 0.6295\n",
            "Epoch [838], Loss: 0.6298\n",
            "Epoch [839], Loss: 0.6308\n",
            "Epoch [840], Loss: 0.6308\n",
            "Persistence: Saving model weights...\n",
            "Epoch [841], Loss: 0.6299\n",
            "Epoch [842], Loss: 0.6287\n",
            "Epoch [843], Loss: 0.6280\n",
            "Epoch [844], Loss: 0.6282\n",
            "Epoch [845], Loss: 0.6286\n",
            "Epoch [846], Loss: 0.6287\n",
            "Epoch [847], Loss: 0.6281\n",
            "Epoch [848], Loss: 0.6272\n",
            "Epoch [849], Loss: 0.6266\n",
            "Epoch [850], Loss: 0.6264\n",
            "Persistence: Saving model weights...\n",
            "Epoch [851], Loss: 0.6266\n",
            "Epoch [852], Loss: 0.6267\n",
            "Epoch [853], Loss: 0.6265\n",
            "Epoch [854], Loss: 0.6261\n",
            "Epoch [855], Loss: 0.6256\n",
            "Epoch [856], Loss: 0.6253\n",
            "Epoch [857], Loss: 0.6252\n",
            "Epoch [858], Loss: 0.6254\n",
            "Epoch [859], Loss: 0.6258\n",
            "Epoch [860], Loss: 0.6264\n",
            "Persistence: Saving model weights...\n",
            "Epoch [861], Loss: 0.6272\n",
            "Epoch [862], Loss: 0.6284\n",
            "Epoch [863], Loss: 0.6300\n",
            "Epoch [864], Loss: 0.6318\n",
            "Epoch [865], Loss: 0.6328\n",
            "Epoch [866], Loss: 0.6324\n",
            "Epoch [867], Loss: 0.6290\n",
            "Epoch [868], Loss: 0.6253\n",
            "Epoch [869], Loss: 0.6233\n",
            "Epoch [870], Loss: 0.6241\n",
            "Persistence: Saving model weights...\n",
            "Epoch [871], Loss: 0.6264\n",
            "Epoch [872], Loss: 0.6283\n",
            "Epoch [873], Loss: 0.6274\n",
            "Epoch [874], Loss: 0.6250\n",
            "Epoch [875], Loss: 0.6226\n",
            "Epoch [876], Loss: 0.6222\n",
            "Epoch [877], Loss: 0.6233\n",
            "Epoch [878], Loss: 0.6242\n",
            "Epoch [879], Loss: 0.6235\n",
            "Epoch [880], Loss: 0.6218\n",
            "Persistence: Saving model weights...\n",
            "Epoch [881], Loss: 0.6202\n",
            "Epoch [882], Loss: 0.6198\n",
            "Epoch [883], Loss: 0.6203\n",
            "Epoch [884], Loss: 0.6207\n",
            "Epoch [885], Loss: 0.6205\n",
            "Epoch [886], Loss: 0.6196\n",
            "Epoch [887], Loss: 0.6187\n",
            "Epoch [888], Loss: 0.6184\n",
            "Epoch [889], Loss: 0.6185\n",
            "Epoch [890], Loss: 0.6187\n",
            "Persistence: Saving model weights...\n",
            "Epoch [891], Loss: 0.6187\n",
            "Epoch [892], Loss: 0.6184\n",
            "Epoch [893], Loss: 0.6179\n",
            "Epoch [894], Loss: 0.6176\n",
            "Epoch [895], Loss: 0.6176\n",
            "Epoch [896], Loss: 0.6179\n",
            "Epoch [897], Loss: 0.6186\n",
            "Epoch [898], Loss: 0.6197\n",
            "Epoch [899], Loss: 0.6216\n",
            "Epoch [900], Loss: 0.6242\n",
            "Persistence: Saving model weights...\n",
            "Epoch [901], Loss: 0.6287\n",
            "Epoch [902], Loss: 0.6329\n",
            "Epoch [903], Loss: 0.6365\n",
            "Epoch [904], Loss: 0.6349\n",
            "Epoch [905], Loss: 0.6275\n",
            "Epoch [906], Loss: 0.6204\n",
            "Epoch [907], Loss: 0.6192\n",
            "Epoch [908], Loss: 0.6230\n",
            "Epoch [909], Loss: 0.6238\n",
            "Epoch [910], Loss: 0.6193\n",
            "Persistence: Saving model weights...\n",
            "Epoch [911], Loss: 0.6157\n",
            "Epoch [912], Loss: 0.6166\n",
            "Epoch [913], Loss: 0.6196\n",
            "Epoch [914], Loss: 0.6183\n",
            "Epoch [915], Loss: 0.6151\n",
            "Epoch [916], Loss: 0.6140\n",
            "Epoch [917], Loss: 0.6157\n",
            "Epoch [918], Loss: 0.6174\n",
            "Epoch [919], Loss: 0.6154\n",
            "Epoch [920], Loss: 0.6129\n",
            "Persistence: Saving model weights...\n",
            "Epoch [921], Loss: 0.6125\n",
            "Epoch [922], Loss: 0.6137\n",
            "Epoch [923], Loss: 0.6146\n",
            "Epoch [924], Loss: 0.6133\n",
            "Epoch [925], Loss: 0.6118\n",
            "Epoch [926], Loss: 0.6115\n",
            "Epoch [927], Loss: 0.6123\n",
            "Epoch [928], Loss: 0.6127\n",
            "Epoch [929], Loss: 0.6120\n",
            "Epoch [930], Loss: 0.6109\n",
            "Persistence: Saving model weights...\n",
            "Epoch [931], Loss: 0.6103\n",
            "Epoch [932], Loss: 0.6105\n",
            "Epoch [933], Loss: 0.6108\n",
            "Epoch [934], Loss: 0.6105\n",
            "Epoch [935], Loss: 0.6099\n",
            "Epoch [936], Loss: 0.6092\n",
            "Epoch [937], Loss: 0.6090\n",
            "Epoch [938], Loss: 0.6090\n",
            "Epoch [939], Loss: 0.6090\n",
            "Epoch [940], Loss: 0.6087\n",
            "Persistence: Saving model weights...\n",
            "Epoch [941], Loss: 0.6082\n",
            "Epoch [942], Loss: 0.6078\n",
            "Epoch [943], Loss: 0.6077\n",
            "Epoch [944], Loss: 0.6077\n",
            "Epoch [945], Loss: 0.6077\n",
            "Epoch [946], Loss: 0.6076\n",
            "Epoch [947], Loss: 0.6075\n",
            "Epoch [948], Loss: 0.6076\n",
            "Epoch [949], Loss: 0.6080\n",
            "Epoch [950], Loss: 0.6090\n",
            "Persistence: Saving model weights...\n",
            "Epoch [951], Loss: 0.6110\n",
            "Epoch [952], Loss: 0.6132\n",
            "Epoch [953], Loss: 0.6172\n",
            "Epoch [954], Loss: 0.6181\n",
            "Epoch [955], Loss: 0.6187\n",
            "Epoch [956], Loss: 0.6134\n",
            "Epoch [957], Loss: 0.6086\n",
            "Epoch [958], Loss: 0.6069\n",
            "Epoch [959], Loss: 0.6087\n",
            "Epoch [960], Loss: 0.6112\n",
            "Persistence: Saving model weights...\n",
            "Epoch [961], Loss: 0.6097\n",
            "Epoch [962], Loss: 0.6070\n",
            "Epoch [963], Loss: 0.6052\n",
            "Epoch [964], Loss: 0.6058\n",
            "Epoch [965], Loss: 0.6071\n",
            "Epoch [966], Loss: 0.6061\n",
            "Epoch [967], Loss: 0.6042\n",
            "Epoch [968], Loss: 0.6032\n",
            "Epoch [969], Loss: 0.6040\n",
            "Epoch [970], Loss: 0.6051\n",
            "Persistence: Saving model weights...\n",
            "Epoch [971], Loss: 0.6046\n",
            "Epoch [972], Loss: 0.6032\n",
            "Epoch [973], Loss: 0.6021\n",
            "Epoch [974], Loss: 0.6022\n",
            "Epoch [975], Loss: 0.6028\n",
            "Epoch [976], Loss: 0.6029\n",
            "Epoch [977], Loss: 0.6023\n",
            "Epoch [978], Loss: 0.6013\n",
            "Epoch [979], Loss: 0.6008\n",
            "Epoch [980], Loss: 0.6010\n",
            "Persistence: Saving model weights...\n",
            "Epoch [981], Loss: 0.6012\n",
            "Epoch [982], Loss: 0.6011\n",
            "Epoch [983], Loss: 0.6005\n",
            "Epoch [984], Loss: 0.6000\n",
            "Epoch [985], Loss: 0.5997\n",
            "Epoch [986], Loss: 0.5996\n",
            "Epoch [987], Loss: 0.5997\n",
            "Epoch [988], Loss: 0.5996\n",
            "Epoch [989], Loss: 0.5993\n",
            "Epoch [990], Loss: 0.5989\n",
            "Persistence: Saving model weights...\n",
            "Epoch [991], Loss: 0.5986\n",
            "Epoch [992], Loss: 0.5985\n",
            "Epoch [993], Loss: 0.5986\n",
            "Epoch [994], Loss: 0.5987\n",
            "Epoch [995], Loss: 0.5989\n",
            "Epoch [996], Loss: 0.5993\n",
            "Epoch [997], Loss: 0.6001\n",
            "Epoch [998], Loss: 0.6018\n",
            "Epoch [999], Loss: 0.6043\n",
            "Finished training. Saving model weights!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Use 'inverse_transform' to convert numerical predictions back to original labels\n",
        "# predicted_labels = model_output.numpy()  # Assuming model_output is a tensor\n",
        "# predicted_categories = encoder.inverse_transform(predicted_labels)"
      ],
      "metadata": {
        "id": "XSlW46OJotBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy of Model"
      ],
      "metadata": {
        "id": "x-ZJTXdVxRY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "def evaluate_accuracy(data_loader, model):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # No gradients needed for evaluation, which saves memory and computations\n",
        "        for data in data_loader:\n",
        "            inputs, labels = data\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "train_accuracy = evaluate_accuracy(train_loader, model)\n",
        "test_accuracy = evaluate_accuracy(test_loader, model)\n",
        "\n",
        "print(f\"Training Accuracy: {train_accuracy:.2f}%\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEiqHVpbxRFS",
        "outputId": "45549a5b-d3c9-4a98-8747-ddfdedfdef47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 81.41%\n",
            "Test Accuracy: 79.90%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prevent Colab from timing out\n",
        "while True: pass"
      ],
      "metadata": {
        "id": "Rx82kGnGTXbv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "e239d549-9f4f-49bd-b1bf-95f02a2ed5c9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-0b244f5a6617>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Prevent Colab from timing out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}